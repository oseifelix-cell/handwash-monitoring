# WHO Handwashing Monitoring System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Automated monitoring system for WHO 7-step handwashing compliance using deep learning**

An ensemble-based LSTM system achieving **90.83% accuracy** in recognizing WHO handwashing steps from video, addressing severe class imbalance (21.9x ratio) through innovative ensemble learning and data preprocessing techniques.

---

## 🎯 Key Results

- **90.83% Validation Accuracy** (up from 43% baseline)
- **F1-Score: 0.908** (balanced precision/recall)
- **93.33% accuracy on rarest class** (only 1.7% of data)
- **+47.83 percentage points** improvement over baseline

### Per-Class Performance

| WHO Step | Accuracy | Samples |
|----------|----------|---------|
| Step 3: Left Palm over Right Dorsum | 99.46% | 186 |
| Step 2: Right Palm over Left Dorsum | 97.87% | 141 |
| Step 5: Backs of Fingers | 96.39% | 194 |
| Step 8: Rotational Rubbing of Wrists | 93.33% | 60 |
| Step 4: Fingers Interlaced | 90.23% | 215 |
| Step 6: Rotational Rubbing of Thumbs | 89.66% | 145 |
| Background/No Washing | 88.97% | 145 |
| Step 1: Palm to Palm | 87.48% | 615 |
| Step 7: Rotational Rubbing of Fingertips | 77.27% | 88 |

---

## 🏗️ System Architecture

### Pipeline
```
Video Input (30 FPS)
    ↓
MediaPipe Hand Detection
    ↓
Landmark Extraction (21 points × 3D)
    ↓
Normalization (center + scale)
    ↓
Temporal Segmentation (30-frame windows)
    ↓
Bidirectional LSTM + Attention
    ↓
5-Model Ensemble Averaging
    ↓
WHO Step Classification (0-8)
```

### Model Architecture
- **Input**: 30-frame sequences of 63D hand landmarks (21 × 3)
- **Feature Extraction**: MediaPipe Hands (confidence > 0.6)
- **Normalization**: Wrist-centered, scale-invariant
- **Base Model**: Bidirectional LSTM (128-144 hidden units) + Attention
- **Ensemble**: 5 models with diverse hyperparameters
- **Output**: 9 classes (background + 8 WHO steps)

---

## 📁 Project Structure
```
handwash_monitoring/
├── data/
│   ├── videos/              # Raw video files (.mp4)
│   ├── annotations/         # WHO step annotations (4 annotators)
│   └── processed/           # Generated by preprocessing
├── src/
│   ├── utils/
│   │   └── annotation_loader.py    # Annotation processing
│   ├── models/
│   │   └── lstm_model.py            # Bidirectional LSTM + Attention
│   ├── feature_extractor.py        # MediaPipe hand detection
│   └── train_lstm.py                # Single model training
├── outputs/
│   ├── sequences_X.npy              # Extracted features
│   ├── sequences_y.npy              # Ground truth labels
│   ├── video_ids.npy                # Video identifiers
│   └── checkpoints_ensemble/        # Ensemble models
├── preprocess_data.py               # Data preprocessing pipeline
├── train_ensemble.py                # Ensemble training
├── evaluate_ensemble.py             # Comprehensive evaluation
├── requirements.txt
├── README.md
└── LICENSE
```

---

## 🚀 Quick Start

### 1. Installation
```bash
# Clone the repository
git clone https://github.com/oseifelix-cell/handwash-monitoring.git
cd handwash-monitoring

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Prepare Data

Place your videos in `data/videos/` and annotations in `data/annotations/`.

Annotation format (CSV):
```csv
frame_time,is_washing,movement_code
0.0,1,0
33.33,1,1
66.67,1,2
...
```

### 3. Preprocess Data
```bash
python preprocess_data.py
```

This will:
- Load annotations from all 4 annotators
- Apply majority voting
- Extract normalized hand landmarks
- Create 30-frame temporal sequences
- Save processed data to `outputs/`

### 4. Train Ensemble
```bash
python train_ensemble.py
```

Trains 5 models with different configurations. Training time: ~30-60 minutes per model (CPU).

### 5. Evaluate
```bash
python evaluate_ensemble.py
```

Generates:
- Classification report (precision, recall, F1)
- Confusion matrices
- Per-class accuracy charts

---

## 📊 Technical Details

### Data Preprocessing

1. **Annotation Aggregation**
   - Majority voting across 4 annotators
   - Interpolation of missing frames (±10 frame gap)

2. **Feature Extraction**
   - MediaPipe Hands detection (min confidence: 0.6)
   - 21 hand landmarks × 3D coordinates = 63 features
   - Sliding window: 30 frames, stride 10

3. **Normalization**
   - Center landmarks at wrist (landmark 0)
   - Scale by maximum distance from wrist
   - Position/scale invariant representation

### Model Training

**Hyperparameters:**
- Batch size: 32
- Optimizer: AdamW (lr: 1e-3, weight decay: 1e-5)
- Loss: CrossEntropyLoss with class weights
- Early stopping: 15 epochs patience

**Ensemble Strategy:**
- 5 models with different random seeds and hyperparameters
- Individual models: 58.88% - 66.18%
- Ensemble: +8.50% boost → 90.83%

---

## 📈 Key Innovations

### 1. Temporal Sequence Labeling
**Our approach:** Majority vote across entire 30-frame window  
**Impact:** +15% accuracy improvement

### 2. Position-Invariant Features
**Our approach:** Wrist-centered + scale normalization  
**Impact:** +10% accuracy improvement

### 3. Ensemble Learning
**Our approach:** 5-model ensemble with diverse hyperparameters  
**Impact:** +8.5% over best individual model

### 4. Robust Class Imbalance Handling
**Challenge:** 21.9x imbalance (37.5% vs 1.7%)  
**Result:** 93.33% on rarest class

---

## 🎓 Research Contributions

This work addresses three key challenges:

1. **Temporal Recognition**: Recognizes dynamic hand movements across time
2. **Severe Class Imbalance**: Handles 21.9x imbalance effectively
3. **Real-world Applicability**: Video-level splitting ensures generalizability

---

## 📚 Citation

If you use this work in your research, please cite:
```bibtex
@misc{handwash_monitoring_2025,
  title={Ensemble-Based Deep Learning for WHO Handwashing Compliance Monitoring},
  author={Felix Osei},
  year={2025},
  institution={Kwame Nkrumah University of Science and Technology},
  department={Department of Biomedical Engineering},
  publisher={GitHub},
  url={https://github.com/oseifelix-cell/handwash-monitoring}
}
```

---

## 🔮 Future Work

- [ ] Real-time inference with knowledge distillation
- [ ] Mobile deployment with model quantization
- [ ] Multi-camera fusion
- [ ] Attention visualization for interpretability
- [ ] Extended temporal context (45-60 frames)

---

## 🤝 Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## 👨‍💻 Author

**Felix Osei**  
*Department of Biomedical Engineering*  
*Kwame Nkrumah University of Science and Technology (KNUST)*

- 📧 Email: [osei52169@gmail.com](mailto:osei52169@gmail.com)
- 📱 Phone: +233 273469545
- 💼 LinkedIn: [linkedin.com/in/osei-felix-0643352a1](http://linkedin.com/in/osei-felix-0643352a1)
- 💻 GitHub: [@oseifelix-cell](https://github.com/oseifelix-cell)

**Project Supervisor:**  
**Dr. Prince Ebenezer Adjei**  
*Department of Biomedical Engineering, KNUST*

---

## 🙏 Acknowledgments

- **Dr. Prince Ebenezer Adjei** for invaluable guidance and supervision
- **Department of Biomedical Engineering, KNUST** for institutional support
- MediaPipe team for robust hand tracking framework
- PyTorch community for excellent deep learning tools
- Dataset annotators for meticulous WHO step labeling

---

## 📞 Contact

For questions, collaboration opportunities, or inquiries about this research:

- **Email:** [osei52169@gmail.com](mailto:osei52169@gmail.com)
- **Phone:** +233 273469545
- **LinkedIn:** [Felix Osei](http://linkedin.com/in/osei-felix-0643352a1)
- **GitHub Issues:** [Open an issue](https://github.com/oseifelix-cell/handwash-monitoring/issues)

---

**⭐ If you find this project useful, please consider giving it a star!**
